{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_post(post, kind):\n",
    "    if kind == 'submission':\n",
    "        post_dict = {}\n",
    "        post_dict['all_awardings'] = post.get('all_awardings', '')\n",
    "        post_dict['allow_live_comments'] = post.get('allow_live_comments', '')\n",
    "        post_dict['author'] = post.get('author', '')\n",
    "        post_dict['author_flair_css_class'] = post.get('author_flair_css_class', '')\n",
    "        post_dict['author_flair_richtext'] = post.get('author_flair_richtext', '')\n",
    "        post_dict['author_flair_text'] = post.get('author_flair_text', '')\n",
    "        post_dict['author_flair_type'] = post.get('author_flair_type', '')\n",
    "        post_dict['author_fullname'] = post.get('author_fullname', '')\n",
    "        post_dict['author_is_blocked'] = post.get('author_is_blocked', '')\n",
    "        post_dict['author_patreon_flair'] = post.get('author_patreon_flair', '')\n",
    "        post_dict['author_premium'] = post.get('author_premium', '')\n",
    "        post_dict['awarders'] = post.get('awarders', '')\n",
    "        post_dict['can_mod_post'] = post.get('can_mod_post', '')\n",
    "        post_dict['content_categories'] = post.get('content_categories', '')\n",
    "        post_dict['contest_mode'] = post.get('contest_mode', '')\n",
    "        post_dict['created_utc'] = post.get('created_utc', '')\n",
    "        post_dict['domain'] = post.get('domain', '')\n",
    "        post_dict['full_link'] = post.get('full_link', '')\n",
    "        post_dict['gildings'] = post.get('gildings', '')\n",
    "        post_dict['id'] = post.get('id', '')\n",
    "        post_dict['is_created_from_ads_ui'] = post.get('is_created_from_ads_ui', '')\n",
    "        post_dict['is_crosspostable'] = post.get('is_crosspostable', '')\n",
    "        post_dict['is_meta'] = post.get('is_meta', '')\n",
    "        post_dict['is_original_content'] = post.get('is_original_content', '')\n",
    "        post_dict['is_reddit_media_domain'] = post.get('is_reddit_media_domain', '')\n",
    "        post_dict['is_robot_indexable'] = post.get('is_robot_indexable', '')\n",
    "        post_dict['is_self'] = post.get('is_self', '')\n",
    "        post_dict['is_video'] = post.get('is_video', '')\n",
    "        post_dict['link_flair_background_color'] = post.get('link_flair_background_color', '')\n",
    "        post_dict['link_flair_css_class'] = post.get('link_flair_css_class', '')\n",
    "        post_dict['link_flair_richtext'] = post.get('link_flair_richtext', '')\n",
    "        post_dict['link_flair_template_id'] = post.get('link_flair_template_id', '')\n",
    "        post_dict['link_flair_text'] = post.get('link_flair_text', '')\n",
    "        post_dict['link_flair_text_color'] = post.get('link_flair_text_color', '')\n",
    "        post_dict['link_flair_type'] = post.get('link_flair_type', '')\n",
    "        post_dict['locked'] = post.get('locked', '')\n",
    "        post_dict['media'] = post.get('media', '')\n",
    "        post_dict['media_embed'] = post.get('media_embed', '')\n",
    "        post_dict['media_only'] = post.get('media_only', '')\n",
    "        post_dict['no_follow'] = post.get('no_follow', '')\n",
    "        post_dict['num_comments'] = post.get('num_comments', '')\n",
    "        post_dict['num_crossposts'] = post.get('num_crossposts', '')\n",
    "        post_dict['over_18'] = post.get('over_18', '')\n",
    "        post_dict['parent_whitelist_status'] = post.get('parent_whitelist_status', '')\n",
    "        post_dict['permalink'] = post.get('permalink', '')\n",
    "        post_dict['pinned'] = post.get('pinned', '')\n",
    "        post_dict['pwls'] = post.get('pwls', '')\n",
    "        post_dict['retrieved_on'] = post.get('retrieved_on', '')\n",
    "        post_dict['score'] = post.get('score', '')\n",
    "        post_dict['selftext'] = post.get('selftext', '')\n",
    "        post_dict['send_replies'] = post.get('send_replies', '')\n",
    "        post_dict['spoiler'] = post.get('spoiler', '')\n",
    "        post_dict['stickied'] = post.get('stickied', '')\n",
    "        post_dict['subreddit'] = post.get('subreddit', '')\n",
    "        post_dict['subreddit_id'] = post.get('subreddit_id', '')\n",
    "        post_dict['subreddit_subscribers'] = post.get('subreddit_subscribers', '')\n",
    "        post_dict['subreddit_type'] = post.get('subreddit_type', '')\n",
    "        post_dict['thumbnail'] = post.get('thumbnail', '')\n",
    "        post_dict['title'] = post.get('title', '')\n",
    "        post_dict['total_awards_received'] = post.get('total_awards_received', '')\n",
    "        post_dict['treatment_tags'] = post.get('treatment_tags', '')\n",
    "        post_dict['upvote_ratio'] = post.get('upvote_ratio', '')\n",
    "        post_dict['url'] = post.get('url', '')\n",
    "        post_dict['whitelist_status'] = post.get('whitelist_status', '')\n",
    "        post_dict['wls'] = post.get('wls', '')\n",
    "    \n",
    "        return post_dict\n",
    "\n",
    "    if kind == 'comment':\n",
    "        post_dict = {}\n",
    "        post_dict['all_awardings'] = post.get('all_awardings', '')\n",
    "        post_dict['archived'] = post.get('archived', '')\n",
    "        post_dict['associated_award'] = post.get('associated_award', '')\n",
    "        post_dict['author'] = post.get('author', '')\n",
    "        post_dict['author_flair_background_color'] = post.get('author_flair_background_color', '')\n",
    "        post_dict['author_flair_css_class'] = post.get('author_flair_css_class', '')\n",
    "        post_dict['author_flair_richtext'] = post.get('author_flair_richtext', '')\n",
    "        post_dict['author_flair_template_id'] = post.get('author_flair_template_id', '')\n",
    "        post_dict['author_flair_text'] = post.get('author_flair_text', '')\n",
    "        post_dict['author_flair_text_color'] = post.get('author_flair_text_color', '')\n",
    "        post_dict['author_flair_type'] = post.get('author_flair_type', '')\n",
    "        post_dict['author_fullname'] = post.get('author_fullname', '')\n",
    "        post_dict['author_patreon_flair'] = post.get('author_patreon_flair', '')\n",
    "        post_dict['author_premium'] = post.get('author_premium', '')\n",
    "        post_dict['body'] = post.get('body', '')\n",
    "        post_dict['body_sha1'] = post.get('body_sha1', '')\n",
    "        post_dict['can_gild'] = post.get('can_gild', '')\n",
    "        post_dict['collapsed'] = post.get('collapsed', '')\n",
    "        post_dict['collapsed_because_crowd_control'] = post.get('collapsed_because_crowd_control', '')\n",
    "        post_dict['collapsed_reason'] = post.get('collapsed_reason', '')\n",
    "        post_dict['collapsed_reason_code'] = post.get('collapsed_reason_code', '')\n",
    "        post_dict['comment_type'] = post.get('comment_type', '')\n",
    "        post_dict['controversiality'] = post.get('controversiality', '')\n",
    "        post_dict['created_utc'] = post.get('created_utc', '')\n",
    "        post_dict['distinguished'] = post.get('distinguished', '')\n",
    "        post_dict['gilded'] = post.get('gilded', '')\n",
    "        post_dict['gildings'] = post.get('gildings', '')\n",
    "        post_dict['id'] = post.get('id', '')\n",
    "        post_dict['is_submitter'] = post.get('is_submitter', '')\n",
    "        post_dict['link_id'] = post.get('link_id', '')\n",
    "        post_dict['locked'] = post.get('locked', '')\n",
    "        post_dict['no_follow'] = post.get('no_follow', '')\n",
    "        post_dict['parent_id'] = post.get('parent_id', '')\n",
    "        post_dict['permalink'] = post.get('permalink', '')\n",
    "        post_dict['retrieved_utc'] = post.get('retrieved_utc', '')\n",
    "        post_dict['score'] = post.get('score', '')\n",
    "        post_dict['score_hidden'] = post.get('score_hidden', '')\n",
    "        post_dict['send_replies'] = post.get('send_replies', '')\n",
    "        post_dict['stickied'] = post.get('stickied', '')\n",
    "        post_dict['subreddit'] = post.get('subreddit', '')\n",
    "        post_dict['subreddit_id'] = post.get('subreddit_id', '')\n",
    "        post_dict['subreddit_name_prefixed'] = post.get('subreddit_name_prefixed', '')\n",
    "        post_dict['subreddit_type'] = post.get('subreddit_type', '')\n",
    "        post_dict['top_awarded_type'] = post.get('top_awarded_type', '')\n",
    "        post_dict['total_awards_received'] = post.get('total_awards_received', '')\n",
    "        post_dict['treatment_tags'] = post.get('treatment_tags', '')\n",
    "        post_dict['unrepliable_reason'] = post.get('unrepliable_reason', '')\n",
    "\n",
    "        return post_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pushshift_query(subreddit, num_loops=10, size=500, kind='submission', before=True, search_time=time.time()):\n",
    "    \n",
    "    # Query Details\n",
    "    path = f'./datasets/{subreddit}_{kind}.csv'\n",
    "    current_time = int(round(search_time, 0))\n",
    "    if before:\n",
    "        frame = 'before'\n",
    "    else:\n",
    "        frame = 'after'\n",
    "\n",
    "    # Grab earliest or latest entry time    \n",
    "    if exists(path) and before:\n",
    "        print('Export file exists. Appending.')\n",
    "        current_time = pd.read_csv(path)['created_utc'].min()\n",
    "        print(current_time)\n",
    "\n",
    "    if exists(path) and not before:\n",
    "        print('Export file exists. Appending.')\n",
    "        current_time = pd.read_csv(path)['created_utc'].max()\n",
    "        # print('Export file does not exist. Generating.')\n",
    "\n",
    "    # Collect data\n",
    "    posts = []\n",
    "    for query in range(num_loops):\n",
    "        # Display       \n",
    "        print(f'Search query: {query + 1}/{num_loops}')\n",
    "        print(f'Subreddit: {subreddit}')\n",
    "        print(f'{frame.title()}: {current_time}')\n",
    "\n",
    "        # Request\n",
    "        url = f'https://api.pushshift.io/reddit/search/{kind}/?subreddit={subreddit}&size={size}&{frame}={current_time}'\n",
    "        res = requests.get(url)\n",
    "\n",
    "        # Return\n",
    "        for post in res.json()['data']:\n",
    "            \n",
    "            try:\n",
    "                post_dict = parse_post(post, kind)\n",
    "                posts.append(post_dict)\n",
    "                current_time = pd.DataFrame(posts)['created_utc'].min()\n",
    "            except:\n",
    "                print('Nope')\n",
    "        \n",
    "        print(f'Current data frame has {len(posts)} rows')\n",
    "        time.sleep(15)\n",
    "\n",
    "    # Compile\n",
    "    posts = pd.DataFrame(posts)\n",
    "    \n",
    "    # Export\n",
    "    if exists(path):\n",
    "        posts.to_csv(path, mode='a', index=False, header=False)\n",
    "    else:\n",
    "        posts.to_csv(path, index=False)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export file exists. Appending.\n",
      "1642888335\n",
      "Search query: 1/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642888335\n",
      "Current data frame has 100 rows\n",
      "Search query: 2/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642888097\n",
      "Current data frame has 200 rows\n",
      "Search query: 3/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642887872\n",
      "Current data frame has 300 rows\n",
      "Search query: 4/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642887653\n",
      "Current data frame has 400 rows\n",
      "Search query: 5/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642887436\n",
      "Current data frame has 500 rows\n",
      "Export file exists. Appending.\n",
      "1642887201\n",
      "Search query: 1/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642887201\n",
      "Current data frame has 100 rows\n",
      "Search query: 2/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642886987\n",
      "Current data frame has 199 rows\n",
      "Search query: 3/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642886787\n",
      "Current data frame has 299 rows\n",
      "Search query: 4/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642886545\n",
      "Current data frame has 399 rows\n",
      "Search query: 5/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642886309\n",
      "Current data frame has 499 rows\n",
      "Export file exists. Appending.\n",
      "1642886081\n",
      "Search query: 1/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642886081\n",
      "Current data frame has 100 rows\n",
      "Search query: 2/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642885833\n",
      "Current data frame has 200 rows\n",
      "Search query: 3/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642885565\n",
      "Current data frame has 300 rows\n",
      "Search query: 4/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642885281\n",
      "Current data frame has 400 rows\n",
      "Search query: 5/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642885062\n",
      "Current data frame has 500 rows\n",
      "Export file exists. Appending.\n",
      "1642884858\n",
      "Search query: 1/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642884858\n",
      "Current data frame has 100 rows\n",
      "Search query: 2/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642884613\n",
      "Current data frame has 200 rows\n",
      "Search query: 3/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642884365\n",
      "Current data frame has 300 rows\n",
      "Search query: 4/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642884145\n",
      "Current data frame has 400 rows\n",
      "Search query: 5/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642883868\n",
      "Current data frame has 500 rows\n",
      "Export file exists. Appending.\n",
      "1642883609\n",
      "Search query: 1/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642883609\n",
      "Current data frame has 99 rows\n",
      "Search query: 2/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642883367\n",
      "Current data frame has 199 rows\n",
      "Search query: 3/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642883095\n",
      "Current data frame has 299 rows\n",
      "Search query: 4/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642882778\n",
      "Current data frame has 399 rows\n",
      "Search query: 5/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642882443\n",
      "Current data frame has 499 rows\n",
      "Export file exists. Appending.\n",
      "1642882163\n",
      "Search query: 1/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642882163\n",
      "Current data frame has 100 rows\n",
      "Search query: 2/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642881896\n",
      "Current data frame has 200 rows\n",
      "Search query: 3/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642881604\n",
      "Current data frame has 300 rows\n",
      "Search query: 4/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642881359\n",
      "Current data frame has 399 rows\n",
      "Search query: 5/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642881106\n",
      "Current data frame has 499 rows\n",
      "Export file exists. Appending.\n",
      "1642880874\n",
      "Search query: 1/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642880874\n",
      "Current data frame has 100 rows\n",
      "Search query: 2/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642880591\n",
      "Current data frame has 199 rows\n",
      "Search query: 3/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642880334\n",
      "Current data frame has 299 rows\n",
      "Search query: 4/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642880073\n",
      "Current data frame has 399 rows\n",
      "Search query: 5/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642879804\n",
      "Current data frame has 499 rows\n",
      "Export file exists. Appending.\n",
      "1642879570\n",
      "Search query: 1/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642879570\n",
      "Current data frame has 100 rows\n",
      "Search query: 2/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642879328\n",
      "Current data frame has 200 rows\n",
      "Search query: 3/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642879081\n",
      "Current data frame has 299 rows\n",
      "Search query: 4/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642878849\n",
      "Current data frame has 399 rows\n",
      "Search query: 5/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642878610\n",
      "Current data frame has 499 rows\n",
      "Export file exists. Appending.\n",
      "1642878370\n",
      "Search query: 1/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642878370\n",
      "Current data frame has 100 rows\n",
      "Search query: 2/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642878096\n",
      "Current data frame has 200 rows\n",
      "Search query: 3/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642877877\n",
      "Current data frame has 300 rows\n",
      "Search query: 4/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642877586\n",
      "Current data frame has 400 rows\n",
      "Search query: 5/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642877341\n",
      "Current data frame has 500 rows\n",
      "Export file exists. Appending.\n",
      "1642877114\n",
      "Search query: 1/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642877114\n",
      "Current data frame has 100 rows\n",
      "Search query: 2/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642876872\n",
      "Current data frame has 200 rows\n",
      "Search query: 3/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642876580\n",
      "Current data frame has 300 rows\n",
      "Search query: 4/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642876315\n",
      "Current data frame has 400 rows\n",
      "Search query: 5/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642876054\n",
      "Current data frame has 500 rows\n",
      "Export file exists. Appending.\n",
      "1642875822\n",
      "Search query: 1/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642875822\n",
      "Current data frame has 100 rows\n",
      "Search query: 2/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642875555\n",
      "Current data frame has 200 rows\n",
      "Search query: 3/5\n",
      "Subreddit: worldnews\n",
      "Before: 1642875299\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3y/cshc407d2zv7031zkk2k4r1r0000gn/T/ipykernel_4848/2394852059.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpushshift_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubreddit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'worldnews'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_loops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'comment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/3y/cshc407d2zv7031zkk2k4r1r0000gn/T/ipykernel_4848/2294488216.py\u001b[0m in \u001b[0;36mpushshift_query\u001b[0;34m(subreddit, num_loops, size, kind, before, search_time)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpost\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    908\u001b[0m                     \u001b[0;31m# used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "for n in range(500):\n",
    "    pushshift_query(subreddit='worldnews', num_loops=5, size=500, kind='comment')\n",
    "    time.sleep(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pushshift/api"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6aaf12f00e2accd7ab3a69f0361120d5b81b11b109d190d2913220c8ee37ccf8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
